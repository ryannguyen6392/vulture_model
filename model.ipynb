{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "571b5705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import feather\n",
    "import csv\n",
    "import pandas\n",
    "import copy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65687e13",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a15cd36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test = [], [], []\n",
    "\n",
    "os.chdir('./Data/training')\n",
    "directory = './'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if 'csv' in filename:\n",
    "        continue\n",
    "    DF = feather.read_dataframe(filename)\n",
    "    X_train.append(DF)\n",
    "X_train = np.stack(X_train, axis=0)\n",
    "y_train = pandas.read_csv('y_train.csv', sep=\",\", header=0,index_col=0).to_numpy().squeeze()\n",
    "    \n",
    "os.chdir('../valid')\n",
    "for filename in os.listdir(directory):\n",
    "    if 'csv' in filename:\n",
    "        continue\n",
    "    DF = feather.read_dataframe(filename)\n",
    "    X_valid.append(DF)\n",
    "X_valid = np.stack(X_valid, axis=0)\n",
    "y_valid = pandas.read_csv('y_valid.csv', sep=\",\", header=0,index_col=0).to_numpy().squeeze()\n",
    "\n",
    "os.chdir('../test')\n",
    "for filename in os.listdir(directory):\n",
    "    if 'csv' in filename:\n",
    "        continue\n",
    "    DF = feather.read_dataframe(filename)\n",
    "    X_test.append(DF)\n",
    "X_test = np.stack(X_test, axis=0)\n",
    "y_test = pandas.read_csv('y_test.csv', sep=\",\", header=0,index_col=0).to_numpy().squeeze()\n",
    "    \n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59223c12",
   "metadata": {},
   "source": [
    "# Check data shape and setup dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "09b1cfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (94, 144, 10)\n",
      "Valid data shape:(36, 144, 10)\n",
      "Test data shape: (26, 144, 10)\n",
      "Train target shape: (94,)\n",
      "Valid target shape: (36,)\n",
      "Test target shape: (26,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train data shape: {X_train.shape}\")\n",
    "print(f\"Valid data shape:{X_valid.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Train target shape: {y_train.shape}\")\n",
    "print(f\"Valid target shape: {y_valid.shape}\")\n",
    "print(f\"Test target shape: {y_test.shape}\")\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y,  transform=None):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.X = torch.tensor(X, dtype=float, device=self.device)\n",
    "        self.y = torch.tensor(y, dtype=torch.long, device=self.device)\n",
    "        self.transform = transform\n",
    "\n",
    "    # def trial_len(self):\n",
    "    #     return self.X.shape[-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x.float(), y\n",
    "\n",
    "batch_size = 1\n",
    "    \n",
    "train_dataset = Dataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size)\n",
    "\n",
    "val_dataset = Dataset(X_valid, y_valid)\n",
    "val_loader = DataLoader(val_dataset, len(X_valid))\n",
    "\n",
    "test_dataset = Dataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, len(X_test))\n",
    "\n",
    "dataloaders = [train_loader, val_loader, test_loader]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1677fb",
   "metadata": {},
   "source": [
    "# Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "53ef89ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, opt, dataloader, loss_fn, curr_epoch, tb_writer=None):\n",
    "    model.train(True)\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    correct = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        opt.zero_grad() # reset opt\n",
    "\n",
    "        X, y = data\n",
    "        outputs = model(X)\n",
    "\n",
    "        loss = loss_fn(outputs, y)\n",
    "        loss.backward() # backprop\n",
    "\n",
    "        # grad step\n",
    "        opt.step()\n",
    "\n",
    "        #training acc\n",
    "        pred = outputs.data.max(1, keepdim=True)[1]\n",
    "        correct += torch.sum(pred.squeeze(1) == y)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            last_loss = loss.item() # loss per batch\n",
    "            tb_x = curr_epoch * len(dataloader) + i + 1\n",
    "\n",
    "            if tb_writer is not None:\n",
    "                tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "\n",
    "            running_loss = 0\n",
    "\n",
    "    final_acc = correct / len(dataloader.dataset)\n",
    "    return last_loss, final_acc\n",
    "\n",
    "def train_and_eval(model, opt, dataloaders, loss_fn, n_epochs, tb_writer=None, verbose=True):\n",
    "    best_vloss = 1_000_000.\n",
    "\n",
    "    train_loader, val_loader, test_loader = dataloaders\n",
    "\n",
    "    best_test_acc = 0\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        avg_loss, train_acc = train_one_epoch(model, opt, train_loader, loss_fn, epoch, tb_writer)\n",
    "\n",
    "        avg_vloss, val_acc = eval(model, val_loader, loss_fn)\n",
    "\n",
    "        avg_tloss, test_acc = eval(model, test_loader, loss_fn)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_test_acc = test_acc\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Epoch: {epoch + 1} LOSS -- train: {avg_loss} valid: {avg_vloss}')\n",
    "            print(f'Epoch: {epoch + 1} Acc -- train: {train_acc} valid: {val_acc} test: {test_acc}')\n",
    "\n",
    "        if tb_writer is not None:\n",
    "            # Log the running loss averaged per batch\n",
    "            # for both training and validation\n",
    "            tb_writer.add_scalars('Training vs. Validation vs. Testing Loss',\n",
    "                            { 'Training' : avg_loss, 'Validation' : avg_vloss, \"Testing\": avg_tloss },\n",
    "                            epoch + 1)\n",
    "\n",
    "            tb_writer.add_scalars('Training vs. Validation vs. Testing Acc',\n",
    "                            { 'Training' : train_acc, 'Validation' : val_acc, \"Testing\": test_acc },\n",
    "                            epoch + 1)\n",
    "            tb_writer.flush()\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print(f\"Training Finished. Best Val Acc: {best_val_acc} Best Test Acc: {best_test_acc}\")\n",
    "    return best_model_state, best_val_acc.item(), best_test_acc.item()\n",
    "\n",
    "def eval(model, dataloader, loss_fn):\n",
    "    running_loss = 0.0\n",
    "    model.eval() # go to evaluation mode\n",
    "\n",
    "    correct = 0\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            inputs, y = data\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            running_loss += loss\n",
    "\n",
    "            pred = outputs.data.max(1, keepdim=True)[1]\n",
    "            correct += torch.sum(pred.squeeze(1) == y)\n",
    "\n",
    "    final_acc = correct / len(dataloader.dataset)\n",
    "\n",
    "    avg_loss = running_loss / (i + 1)\n",
    "\n",
    "    return avg_loss, final_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f288c6f",
   "metadata": {},
   "source": [
    "# Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c6ee1b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=.4):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.CNN = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=10, out_channels=10, kernel_size=2, padding='same'),\n",
    "            nn.AvgPool1d(kernel_size=2,padding=0),\n",
    "            nn.InstanceNorm1d(num_features = 10, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ELU(inplace = True),\n",
    "            nn.Dropout(p=dropout),\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(10, 10, 1, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(10, 10),\n",
    "            nn.InstanceNorm1d(num_features=10, eps=1e-05, momentum=0.2, affine=False),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(720, 2),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        N, L, H = x.size() # (1, 144, 10)\n",
    "        x = x.view(N, H, L) # reshape to (N, H, L) to fit CNN\n",
    "\n",
    "        out= self.CNN(x) # output is (N, H, L/2)\n",
    "        out = out.view(out.shape[0], out.size(2), out.size(1)) # reshape to (N, L, H)\n",
    "        out = self.lstm(out)\n",
    "        out = self.fc(out[0])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb74f7",
   "metadata": {},
   "source": [
    "# Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4cc04517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryann\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n",
      "  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n",
      "C:\\Users\\ryann\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 LOSS -- train: 0.6033260822296143 valid: 0.7203848361968994\n",
      "Epoch: 1 Acc -- train: 0.5106382966041565 valid: 0.472222238779068 test: 0.5\n",
      "Epoch: 2 LOSS -- train: 0.5355368852615356 valid: 0.7025401592254639\n",
      "Epoch: 2 Acc -- train: 0.5106382966041565 valid: 0.3611111044883728 test: 0.5\n",
      "Epoch: 3 LOSS -- train: 0.6471095681190491 valid: 0.7006485462188721\n",
      "Epoch: 3 Acc -- train: 0.4999999701976776 valid: 0.5 test: 0.6153846383094788\n",
      "Epoch: 4 LOSS -- train: 0.7018797397613525 valid: 0.701759934425354\n",
      "Epoch: 4 Acc -- train: 0.44680848717689514 valid: 0.5277777910232544 test: 0.5384615659713745\n",
      "Epoch: 5 LOSS -- train: 0.7277728319168091 valid: 0.7033064365386963\n",
      "Epoch: 5 Acc -- train: 0.45744678378105164 valid: 0.5833333134651184 test: 0.5384615659713745\n",
      "Epoch: 6 LOSS -- train: 0.7427035570144653 valid: 0.7050879001617432\n",
      "Epoch: 6 Acc -- train: 0.4787233769893646 valid: 0.5833333134651184 test: 0.5\n",
      "Epoch: 7 LOSS -- train: 0.7523221969604492 valid: 0.7068971991539001\n",
      "Epoch: 7 Acc -- train: 0.4787233769893646 valid: 0.5833333134651184 test: 0.5\n",
      "Epoch: 8 LOSS -- train: 0.7613480091094971 valid: 0.7085862755775452\n",
      "Epoch: 8 Acc -- train: 0.5106382966041565 valid: 0.5555555820465088 test: 0.5\n",
      "Epoch: 9 LOSS -- train: 0.7705061435699463 valid: 0.7102108001708984\n",
      "Epoch: 9 Acc -- train: 0.521276593208313 valid: 0.5833333134651184 test: 0.5\n",
      "Epoch: 10 LOSS -- train: 0.779751718044281 valid: 0.7118710875511169\n",
      "Epoch: 10 Acc -- train: 0.563829779624939 valid: 0.5833333134651184 test: 0.5\n",
      "Epoch: 11 LOSS -- train: 0.7896575927734375 valid: 0.7134150266647339\n",
      "Epoch: 11 Acc -- train: 0.5531914830207825 valid: 0.5555555820465088 test: 0.5384615659713745\n",
      "Epoch: 12 LOSS -- train: 0.7987496852874756 valid: 0.7150279879570007\n",
      "Epoch: 12 Acc -- train: 0.5531914830207825 valid: 0.5555555820465088 test: 0.5384615659713745\n",
      "Epoch: 13 LOSS -- train: 0.8100335597991943 valid: 0.716167688369751\n",
      "Epoch: 13 Acc -- train: 0.5744680762290955 valid: 0.5555555820465088 test: 0.5384615659713745\n",
      "Epoch: 14 LOSS -- train: 0.8216016292572021 valid: 0.7171424627304077\n",
      "Epoch: 14 Acc -- train: 0.585106372833252 valid: 0.5555555820465088 test: 0.5384615659713745\n",
      "Epoch: 15 LOSS -- train: 0.8304187059402466 valid: 0.7181143760681152\n",
      "Epoch: 15 Acc -- train: 0.5957446694374084 valid: 0.5555555820465088 test: 0.5384615659713745\n",
      "Epoch: 16 LOSS -- train: 0.8349811434745789 valid: 0.7190240621566772\n",
      "Epoch: 16 Acc -- train: 0.6276595592498779 valid: 0.5555555820465088 test: 0.5384615659713745\n",
      "Epoch: 17 LOSS -- train: 0.835808515548706 valid: 0.7196837663650513\n",
      "Epoch: 17 Acc -- train: 0.6382978558540344 valid: 0.5555555820465088 test: 0.5384615659713745\n",
      "Epoch: 18 LOSS -- train: 0.8359148502349854 valid: 0.7202393412590027\n",
      "Epoch: 18 Acc -- train: 0.6382978558540344 valid: 0.5555555820465088 test: 0.5\n",
      "Epoch: 19 LOSS -- train: 0.8334295153617859 valid: 0.7204631567001343\n",
      "Epoch: 19 Acc -- train: 0.6595744490623474 valid: 0.5555555820465088 test: 0.5\n",
      "Epoch: 20 LOSS -- train: 0.8293542861938477 valid: 0.7207241654396057\n",
      "Epoch: 20 Acc -- train: 0.6702127456665039 valid: 0.5555555820465088 test: 0.5384615659713745\n",
      "Epoch: 21 LOSS -- train: 0.8223292827606201 valid: 0.7210680246353149\n",
      "Epoch: 21 Acc -- train: 0.6914893388748169 valid: 0.5555555820465088 test: 0.5384615659713745\n",
      "Epoch: 22 LOSS -- train: 0.813953161239624 valid: 0.721743643283844\n",
      "Epoch: 22 Acc -- train: 0.6808510422706604 valid: 0.5833333134651184 test: 0.5384615659713745\n",
      "Epoch: 23 LOSS -- train: 0.8030240535736084 valid: 0.7222166657447815\n",
      "Epoch: 23 Acc -- train: 0.6808510422706604 valid: 0.5833333134651184 test: 0.5384615659713745\n",
      "Epoch: 24 LOSS -- train: 0.7907336354255676 valid: 0.7226593494415283\n",
      "Epoch: 24 Acc -- train: 0.6914893388748169 valid: 0.5833333134651184 test: 0.5384615659713745\n",
      "Epoch: 25 LOSS -- train: 0.7763678431510925 valid: 0.7230435013771057\n",
      "Epoch: 25 Acc -- train: 0.6914893388748169 valid: 0.5833333134651184 test: 0.5384615659713745\n",
      "Epoch: 26 LOSS -- train: 0.7615361213684082 valid: 0.723314106464386\n",
      "Epoch: 26 Acc -- train: 0.7021276354789734 valid: 0.5555555820465088 test: 0.5384615659713745\n",
      "Epoch: 27 LOSS -- train: 0.7469667792320251 valid: 0.7239270806312561\n",
      "Epoch: 27 Acc -- train: 0.7234042286872864 valid: 0.5555555820465088 test: 0.5384615659713745\n",
      "Epoch: 28 LOSS -- train: 0.7296929955482483 valid: 0.7246568202972412\n",
      "Epoch: 28 Acc -- train: 0.7234042286872864 valid: 0.5555555820465088 test: 0.46153849363327026\n",
      "Epoch: 29 LOSS -- train: 0.7118141651153564 valid: 0.725625216960907\n",
      "Epoch: 29 Acc -- train: 0.7553191184997559 valid: 0.5555555820465088 test: 0.46153849363327026\n",
      "Epoch: 30 LOSS -- train: 0.6942368745803833 valid: 0.7267246246337891\n",
      "Epoch: 30 Acc -- train: 0.7765957117080688 valid: 0.5555555820465088 test: 0.46153849363327026\n",
      "Epoch: 31 LOSS -- train: 0.6750471591949463 valid: 0.7276356816291809\n",
      "Epoch: 31 Acc -- train: 0.7978723049163818 valid: 0.5277777910232544 test: 0.5\n",
      "Epoch: 32 LOSS -- train: 0.6570742130279541 valid: 0.728742241859436\n",
      "Epoch: 32 Acc -- train: 0.7978723049163818 valid: 0.5277777910232544 test: 0.5\n",
      "Epoch: 33 LOSS -- train: 0.6392076015472412 valid: 0.7296269536018372\n",
      "Epoch: 33 Acc -- train: 0.7978723049163818 valid: 0.5277777910232544 test: 0.5\n",
      "Epoch: 34 LOSS -- train: 0.6236792206764221 valid: 0.7304826378822327\n",
      "Epoch: 34 Acc -- train: 0.8085106015205383 valid: 0.5555555820465088 test: 0.46153849363327026\n",
      "Epoch: 35 LOSS -- train: 0.6093397736549377 valid: 0.73129802942276\n",
      "Epoch: 35 Acc -- train: 0.8297871947288513 valid: 0.5555555820465088 test: 0.42307692766189575\n",
      "Epoch: 36 LOSS -- train: 0.5958977937698364 valid: 0.7320979833602905\n",
      "Epoch: 36 Acc -- train: 0.8297871947288513 valid: 0.5555555820465088 test: 0.42307692766189575\n",
      "Epoch: 37 LOSS -- train: 0.5825465321540833 valid: 0.7331481575965881\n",
      "Epoch: 37 Acc -- train: 0.8297871947288513 valid: 0.5833333134651184 test: 0.42307692766189575\n",
      "Epoch: 38 LOSS -- train: 0.5665895938873291 valid: 0.7341481447219849\n",
      "Epoch: 38 Acc -- train: 0.8297871947288513 valid: 0.5833333134651184 test: 0.42307692766189575\n",
      "Epoch: 39 LOSS -- train: 0.5530867576599121 valid: 0.7346810698509216\n",
      "Epoch: 39 Acc -- train: 0.8297871947288513 valid: 0.5555555820465088 test: 0.42307692766189575\n",
      "Epoch: 40 LOSS -- train: 0.5423920750617981 valid: 0.7357377409934998\n",
      "Epoch: 40 Acc -- train: 0.8297871947288513 valid: 0.5555555820465088 test: 0.42307692766189575\n",
      "Epoch: 41 LOSS -- train: 0.5301698446273804 valid: 0.7360901236534119\n",
      "Epoch: 41 Acc -- train: 0.8510637879371643 valid: 0.5555555820465088 test: 0.42307692766189575\n",
      "Epoch: 42 LOSS -- train: 0.5211153030395508 valid: 0.7379750609397888\n",
      "Epoch: 42 Acc -- train: 0.8510637879371643 valid: 0.5277777910232544 test: 0.42307692766189575\n",
      "Epoch: 43 LOSS -- train: 0.5049415826797485 valid: 0.7394682168960571\n",
      "Epoch: 43 Acc -- train: 0.8617020845413208 valid: 0.5 test: 0.42307692766189575\n",
      "Epoch: 44 LOSS -- train: 0.4937009811401367 valid: 0.7407767176628113\n",
      "Epoch: 44 Acc -- train: 0.8723403811454773 valid: 0.472222238779068 test: 0.42307692766189575\n",
      "Epoch: 45 LOSS -- train: 0.4833219647407532 valid: 0.7414128184318542\n",
      "Epoch: 45 Acc -- train: 0.8617020845413208 valid: 0.472222238779068 test: 0.42307692766189575\n",
      "Epoch: 46 LOSS -- train: 0.4764305353164673 valid: 0.7423622608184814\n",
      "Epoch: 46 Acc -- train: 0.8617020845413208 valid: 0.472222238779068 test: 0.42307692766189575\n",
      "Epoch: 47 LOSS -- train: 0.4693553149700165 valid: 0.743429958820343\n",
      "Epoch: 47 Acc -- train: 0.8617020845413208 valid: 0.472222238779068 test: 0.42307692766189575\n",
      "Epoch: 48 LOSS -- train: 0.4610394835472107 valid: 0.7446941137313843\n",
      "Epoch: 48 Acc -- train: 0.8617020845413208 valid: 0.472222238779068 test: 0.42307692766189575\n",
      "Epoch: 49 LOSS -- train: 0.45257118344306946 valid: 0.7463293671607971\n",
      "Epoch: 49 Acc -- train: 0.8936169743537903 valid: 0.472222238779068 test: 0.42307692766189575\n",
      "Epoch: 50 LOSS -- train: 0.44426724314689636 valid: 0.74852454662323\n",
      "Epoch: 50 Acc -- train: 0.9042552709579468 valid: 0.4444444477558136 test: 0.42307692766189575\n",
      "Training Finished. Best Val Acc: 0.5833333134651184 Best Test Acc: 0.5384615659713745\n"
     ]
    }
   ],
   "source": [
    "LR      = 0.0001\n",
    "BETAS   = (0.9, 0.999)\n",
    "EPS     = 1e-08\n",
    "DECAY   = 0.005\n",
    "DROPOUT = 0\n",
    "\n",
    "EPOCHS  = 50\n",
    "CRITERION = nn.CrossEntropyLoss()\n",
    "\n",
    "model = Model(dropout=DROPOUT).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, betas=BETAS, eps=EPS, weight_decay=DECAY)\n",
    "best_model = train_and_eval(model, optimizer, dataloaders, CRITERION, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75863f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e4c00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
